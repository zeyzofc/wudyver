import fetch from "node-fetch";
import axios from "axios";
import * as cheerio from "cheerio";
import _ from "lodash";
import {
  v4 as uuid
} from "uuid";
class Aichat {
  constructor() {
    this.url = "https://chat-gpt.org/chat", this.working = !0, this.supports_gpt_35_turbo = !0;
  }
  async createAsync(model, messages, kwargs = {}) {
    const json_data = {
        message: this.formatPrompt(messages),
        temperature: kwargs.temperature || .5,
        presence_penalty: 0,
        top_p: kwargs.top_p || 1,
        frequency_penalty: 0
      },
      response = await fetch("https://chat-gpt.org/api/text", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36"
        },
        body: JSON.stringify(json_data)
      });
    if (!response.ok) throw new Error(`HTTP Error: ${response.status}`);
    const result = await response.json();
    if (!result.response) throw new Error(`Error Response: ${JSON.stringify(result)}`);
    return result.message;
  }
  formatPrompt(messages) {
    return JSON.stringify(messages);
  }
}
class AI {
  constructor() {
    this.axiosInstance = axios.create({
      baseURL: "https://gke-prod-api.useadrenaline.com/",
      headers: {
        Accept: "application/json, text/plain, */*",
        "Content-Type": "application/json",
        "x-instance": "adrenaline"
      }
    });
    this.api = axios.create({
      baseURL: "https://search.lepton.run/api/",
      headers: {
        "Content-Type": "application/json"
      }
    });
    this.headersSimSimi = {
      "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
      Accept: "application/json, text/javascript, */*; q=0.01",
      "X-Requested-With": "XMLHttpRequest",
      "User-Agent": "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Mobile Safari/537.36",
      Referer: "https://simsimi.vn/"
    };
    this.headersGoody = {
      Accept: "*/*",
      "Accept-Encoding": "gzip, deflate, br, zstd",
      "Accept-Language": "id-ID,id;q=0.9,en-US;q=0.8,en;q=0.7,af;q=0.6",
      "Content-Type": "application/json",
      Origin: "https://www.goody2.ai",
      Referer: "https://www.goody2.ai/chat",
      "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
    };
  }
  useadrenaline = async q => {
    try {
      const data = {
        title: q,
        body: "",
        snippets: [],
        is_rush_enabled: false,
        is_public: false,
        files: []
      };
      const {
        data: postResponseData
      } = await this.axiosInstance.post("question", data);
      const {
        data: threadResponseData
      } = await this.axiosInstance.get(`thread/${postResponseData.question_id}?page=1&per_page=10`);
      let jobStatus = "IN_PROGRESS";
      let dataHasil = null;
      while (jobStatus === "IN_PROGRESS") {
        const {
          data: answersResponseData
        } = await this.axiosInstance.get(`question/${threadResponseData.list[0]?.question?.id}/answers`);
        jobStatus = answersResponseData?.[0]?.job_status || "NOT_FOUND";
        dataHasil = answersResponseData?.[0]?.content;
        if (jobStatus === "IN_PROGRESS") {
          console.log("Job is still in progress...");
          await new Promise(resolve => setTimeout(resolve, 1e3));
        }
      }
      return dataHasil;
    } catch (error) {
      console.error("Error fetching useadrenaline data:", error);
      throw error;
    }
  };
  LetmeGpt = async query => {
    const encodedQuery = encodeURIComponent(query);
    const url = `https://letmegpt.com/search?q=${encodedQuery}`;
    try {
      const response = await axios.get(url);
      const $ = cheerio.load(response.data);
      return $("#gptans").text() || null;
    } catch (error) {
      console.error("Error fetching LetmeGpt data:", error);
      throw error;
    }
  };
  generateRandomID = length => {
    const characters = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz";
    return _.sampleSize(characters, length).join("");
  };
  leptonAi = async query => {
    try {
      const rid = this.generateRandomID(10);
      const postData = {
        query: query,
        rid: rid
      };
      const response = await this.api.post("query", postData);
      const llmResponseRegex = /__LLM_RESPONSE__([\s\S]*?)__RELATED_QUESTIONS__/;
      const llmResponseMatch = response.data.match(llmResponseRegex);
      if (llmResponseMatch?.[1]) {
        let llmResponse = llmResponseMatch[1].trim();
        return llmResponse;
      } else {
        throw new Error("No LLM response found.");
      }
    } catch (error) {
      console.error("Error fetching leptonAi response:", error);
      throw new Error("Error fetching LLM response: " + error.message);
    }
  };
  Simsimi = async text => {
    const url = "https://simsimi.vn/web/simtalk";
    try {
      const response = await axios.post(url, `text=${encodeURIComponent(text)}&lc=id`, {
        headers: this.headersSimSimi
      });
      return response.data.success || null;
    } catch (error) {
      console.error("Error asking SimSimi:", error);
      throw error;
    }
  };
  GoodyAI = async q => {
    try {
      const params = {
        message: q,
        debugParams: null
      };
      const response = await axios.post("https://www.goody2.ai/send", params, {
        headers: this.headersGoody,
        responseType: "stream"
      });
      return new Promise((resolve, reject) => {
        let fullText = "";
        response.data.on("data", chunk => {
          const lines = chunk.toString().split("\n");
          for (let line of lines) {
            if (line.startsWith('data: {"content":')) {
              try {
                const content = JSON.parse(line.slice(6)).content;
                fullText += content;
              } catch (err) {
                console.error("Error parsing JSON:", err);
              }
            }
          }
        });
        response.data.on("end", () => {
          resolve(fullText);
        });
        response.data.on("error", err => {
          reject(err);
        });
      });
    } catch (error) {
      console.error("Error asking GoodyAI:", error);
      throw error;
    }
  };
  CgtAi = async text => {
    try {
      const conversation_uuid = uuid();
      const requestData = {
        conversation_uuid: conversation_uuid,
        text: text,
        sent_messages: 1
      };
      const config = {
        headers: {
          "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
          Accept: "*/*",
          "X-Requested-With": "XMLHttpRequest"
        }
      };
      const response = await axios.post("https://www.timospecht.de/wp-json/cgt/v1/chat", JSON.stringify(requestData), config);
      return response.data?.data?.message;
    } catch (error) {
      console.error("Error fetching CgtAi data:", error);
      throw new Error("Terjadi kesalahan:", error);
    }
  };
  thinkany = async prompt => {
    try {
      const response = await axios.post("https://thinkany.ai/api/chat", {
        role: "user",
        content: prompt,
        conv_uuid: uuid(),
        mode: "search",
        is_new: true,
        model: "claude-3-haiku"
      }, {
        headers: {
          "Content-Type": "application/json"
        }
      });
      return response.data;
    } catch (error) {
      console.error("Error fetching thinkany data:", error);
      throw error;
    }
  };
  degreeguru = async message => {
    try {
      const response = await fetch("https://degreeguru.vercel.app/api/guru", {
        method: "POST",
        headers: {
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          messages: [{
            role: "user",
            content: message
          }]
        })
      });
      if (!response.ok) {
        throw new Error("Network response was not ok");
      }
      const responseData = await response.text();
      return responseData;
    } catch (error) {
      console.error("Error calling Degree Guru API:", error.message);
      throw error;
    }
  };
  ragbot = async message => {
    try {
      const response = await fetch("https://ragbot-starter.vercel.app/api/chat", {
        method: "POST",
        headers: {
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          messages: [{
            role: "user",
            content: message
          }],
          useRag: true,
          llm: "gpt-3.5-turbo",
          similarityMetric: "cosine"
        })
      });
      if (!response.ok) {
        throw new Error("Network response was not ok");
      }
      const responseData = await response.text();
      return responseData;
    } catch (error) {
      console.error("Error calling Degree Guru API:", error.message);
      throw error;
    }
  };
  stoicai = async query => {
    try {
      const response = await fetch("https://app.stoiccord.com/api/completion", {
        method: "POST",
        headers: {
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          messages: [{
            role: "user",
            content: query
          }]
        })
      });
      if (!response.ok) {
        throw new Error("Network response was not ok");
      }
      const responseData = await response.text();
      return responseData;
    } catch (error) {
      console.error("Error calling stoic API:", error.message);
      throw error;
    }
  };
  stoicgpt = async query => {
    try {
      const response = await fetch("https://app.stoiccord.com/api/chat", {
        method: "POST",
        headers: {
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          messages: [{
            role: "user",
            content: query
          }]
        })
      });
      if (!response.ok) {
        throw new Error("Network response was not ok");
      }
      const responseData = await response.text();
      return responseData;
    } catch (error) {
      console.error("Error calling stoic API:", error.message);
      throw error;
    }
  };
  alicia = async q => {
    try {
      const response = await axios.get("https://yw85opafq6.execute-api.us-east-1.amazonaws.com/default/boss_mode_15aug", {
        params: {
          text: `Generate humanized chatgpt text in Indonesian, you are an AI assistant named Alicia programmed with Alicia 3 version for the following text: ${q}`,
          country: "Asia",
          user_id: "k2r4gMUJfN"
        }
      });
      return response.data;
    } catch (error) {
      console.error(error);
    }
  };
  aichat = async (q, model) => {
    try {
      const ai = new Aichat();
      const result = await ai.createAsync([{
        role: "user",
        content: q
      }], model);
      return result;
    } catch (error) {
      console.error(error);
    }
  };
  cairo = async q => {
    try {
      const response = await fetch("https://cairo-chatbot.vercel.app/api/chat", {
        method: "POST",
        headers: {
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          messages: [{
            role: "assistant",
            content: "Saya AI dari OpenAI, diciptakan untuk membantu Anda mengeksplorasi ide, bertukar informasi, dan menyelesaikan masalah. Ada yang bisa saya bantu?"
          }, {
            role: "user",
            content: q
          }],
          previewToken: null
        })
      });
      if (!response.ok) {
        throw new Error(`HTTP error! Status: ${response.status}`);
      }
      const result = await response.text();
      return result;
    } catch (error) {
      console.error("Fetch error:", error);
    }
  };
  omniplexAi = async (prompt, system = "You are an Ai Assistant that is designated to help users with their problems") => {
    try {
      const BASE_URL = "https://omniplex.ai/api";
      const headers = {
        origin: BASE_URL.replace("/api", ""),
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
        "Content-Type": "application/json"
      };
      const chatJSON = {
        frequency_penalty: 0,
        max_tokens: 512,
        messages: [{
          role: "system",
          content: system
        }, {
          role: "user",
          content: prompt
        }],
        model: "gpt-3.5-turbo",
        presence_penalty: 0,
        temperature: 1,
        top_p: 1
      };
      const {
        mode,
        arg
      } = await fetch(`${BASE_URL}/tools`, {
        method: "POST",
        headers: headers,
        body: JSON.stringify(chatJSON.messages)
      }).then(res => res.json());
      const chat = async () => {
        try {
          const response = await fetch(`${BASE_URL}/chat`, {
            method: "POST",
            headers: headers,
            body: JSON.stringify(chatJSON)
          }).then(res => res.text());
          return response ? [true, response] : [false, "Failed to get result"];
        } catch (error) {
          return [false, error];
        }
      };
      const searchMode = async () => {
        try {
          const searchResponse = await fetch(`${BASE_URL}/search?${new URLSearchParams({
q: `search ${prompt}`,
limit: 5
})}`).then(res => res.json());
          if (searchResponse.message !== "Success") return [false, "Failed to search"];
          const urls = searchResponse.data.webPages.value.map(v => v.url);
          const scrapedData = await fetch(`${BASE_URL}/scrape?${new URLSearchParams({
urls: urls.join(",")
})}`, {
            method: "POST",
            headers: headers
          }).then(res => res.text());
          chatJSON.messages[1].content = `${scrapedData}\n\nQuestion : ${prompt}`;
          chatJSON.messages[0].content = `Generate a comprehensive and informative answer (but no more than 256 words in 2 paragraphs) for a given question solely based on the provided web Search Results (URL and Summary). You must only use information from the provided search results. Use an unbiased and journalistic tone. Use this current date and time: ${new Date().toUTCString()}. Combine search results together into a coherent answer. Do not repeat text. Only cite the most relevant results that answer the question accurately. If different results refer to different entities with the same name, write separate answers for each entity. You have the ability to search and will be given websites and the scraped data from them and you will have to make up an answer with that only. ${system}`;
          const chatResponse = await fetch(`${BASE_URL}/chat`, {
            method: "POST",
            headers: headers,
            body: JSON.stringify(chatJSON)
          }).then(res => res.text());
          return [true, chatResponse, searchResponse.data];
        } catch (error) {
          return [false, error];
        }
      };
      const result = mode === "search" ? await searchMode() : await chat();
      if (!result[0]) throw new Error(`Mode failed with error: \n${result[1]}`);
      return {
        mode: mode,
        data: result[1],
        search: result[2]
      };
    } catch (error) {
      throw new Error(error);
    }
  };
}
export default async function handler(req, res) {
  const {
    prompt,
    model = "aichat"
  } = req.method === "GET" ? req.query : req.body;
  if (!prompt) {
    return res.status(400).json({
      error: "Parameter 'prompt' diperlukan."
    });
  }
  const supportedModels = ["CgtAi", "GoodyAI", "LetmeGpt", "Simsimi", "alicia", "cairo", "degreeguru", "leptonAi", "omniplexAi", "ragbot", "stoicai", "stoicgpt", "thinkany", "useadrenaline", "aichat"];
  if (!supportedModels.includes(model)) {
    return res.status(400).json({
      error: `Model tidak valid. Pilih salah satu: ${supportedModels.join(", ")}.`
    });
  }
  try {
    const ai = new AI();
    if (typeof ai[model] !== "function") {
      throw new Error(`Model ${model} tidak memiliki implementasi fungsi.`);
    }
    const result = await ai[model](prompt);
    return res.status(200).json({
      result: result
    });
  } catch (error) {
    console.error("Error processing request:", error.message);
    return res.status(500).json({
      error: "Terjadi kesalahan pada server.",
      detail: error.message
    });
  }
}